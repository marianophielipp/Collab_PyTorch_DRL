{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of stable_baselines_getting_started.ipynb","version":"0.3.2","provenance":[{"file_id":"1_1H5bjWKYBVKbbs-Kj83dsfuZieDNcFU","timestamp":1541340549434}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"hyyN-2qyK_T2","colab_type":"text"},"cell_type":"markdown","source":["# Stable Baselines, a Fork of OpenAI Baselines - Getting Started\n","\n","Github Repo: [https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)\n","\n","Medium article: [https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82](https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82)\n","\n","## Install Dependencies and Stable Baselines Using Pip\n","\n","List of full dependencies can be found in the [README](https://github.com/hill-a/stable-baselines).\n","\n","```\n","\n","sudo apt-get update && sudo apt-get install cmake libopenmpi-dev zlib1g-dev\n","```\n","\n","\n","```\n","\n","pip install stable-baselines\n","```"]},{"metadata":{"id":"gWskDE2c9WoN","colab_type":"code","colab":{}},"cell_type":"code","source":["!apt install cmake libopenmpi-dev zlib1g-dev\n","!pip install stable-baselines==2.1.1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FtY8FhliLsGm","colab_type":"text"},"cell_type":"markdown","source":["## Import policy, RL agent, ..."]},{"metadata":{"id":"BIedd7Pz9sOs","colab_type":"code","colab":{}},"cell_type":"code","source":["import gym\n","import numpy as np\n","\n","from stable_baselines.common.policies import MlpPolicy\n","from stable_baselines.common.vec_env import DummyVecEnv\n","from stable_baselines.ppo2 import PPO2"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RapkYvTXL7Cd","colab_type":"text"},"cell_type":"markdown","source":["## Create the Gym env and instantiate the agent\n","\n","For this example, we will use CartPole environment, a classic control problem.\n","\n","\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n","\n","Cartpole environment: [https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/)\n","\n","![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)\n","\n","Note: vectorized environments allow to easily multiprocess training. In this example, we are using only one process, hence the DummyVecEnv.\n","\n","We chose the MlpPolicy because input of CartPole is a feature vector, not images.\n","\n","The type of action to use (discrete/continuous) will be automatically deduced from the environment action space\n","\n"]},{"metadata":{"id":"pUWGZp3i9wyf","colab_type":"code","outputId":"bc4f0977-cfdb-4911-9bda-2aab1127587e","executionInfo":{"status":"ok","timestamp":1540023692978,"user_tz":-120,"elapsed":1471,"user":{"displayName":"Antonin Raffin","photoUrl":"","userId":"01724440157307517280"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["env = gym.make('CartPole-v1')\n","# vectorized environments allow to easily multiprocess training\n","# we demonstrate its usefulness in the next examples\n","env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run\n","\n","model = PPO2(MlpPolicy, env, verbose=0)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"],"name":"stdout"}]},{"metadata":{"id":"4efFdrQ7MBvl","colab_type":"text"},"cell_type":"markdown","source":["We create a helper function to evaluate the agent:"]},{"metadata":{"id":"63M8mSKR-6Zt","colab_type":"code","colab":{}},"cell_type":"code","source":["def evaluate(model, num_steps=1000):\n","  \"\"\"\n","  Evaluate a RL agent\n","  :param model: (BaseRLModel object) the RL Agent\n","  :param num_steps: (int) number of timesteps to evaluate it\n","  :return: (float) Mean reward for the last 100 episodes\n","  \"\"\"\n","  episode_rewards = [0.0]\n","  obs = env.reset()\n","  for i in range(num_steps):\n","      # _states are only useful when using LSTM policies\n","      action, _states = model.predict(obs)\n","      # here, action, rewards and dones are arrays\n","      # because we are using vectorized env\n","      obs, rewards, dones, info = env.step(action)\n","      \n","      # Stats\n","      episode_rewards[-1] += rewards[0]\n","      if dones[0]:\n","          obs = env.reset()\n","          episode_rewards.append(0.0)\n","  # Compute mean reward for the last 100 episodes\n","  mean_100ep_reward = round(np.mean(episode_rewards[-100:]), 1)\n","  print(\"Mean reward:\", mean_100ep_reward, \"Num episodes:\", len(episode_rewards))\n","  \n","  return mean_100ep_reward"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zjEVOIY8NVeK","colab_type":"text"},"cell_type":"markdown","source":["Let's evaluate the un-trained agent, this should be a random agent."]},{"metadata":{"id":"xDHLMA6NFk95","colab_type":"code","outputId":"edac26ef-44e4-4722-9076-f95ccc86ba0a","executionInfo":{"status":"ok","timestamp":1540023700203,"user_tz":-120,"elapsed":6433,"user":{"displayName":"Antonin Raffin","photoUrl":"","userId":"01724440157307517280"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["# Random Agent, before training\n","mean_reward_before_train = evaluate(model, num_steps=10000)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mean reward: 21.6 Num episodes: 461\n"],"name":"stdout"}]},{"metadata":{"id":"r5UoXTZPNdFE","colab_type":"text"},"cell_type":"markdown","source":["## Train the agent and evaluate it"]},{"metadata":{"id":"e4cfSXIB-pTF","colab_type":"code","outputId":"4b105ae6-fb11-4de6-b546-52bdda0b7a35","executionInfo":{"status":"ok","timestamp":1540023709006,"user_tz":-120,"elapsed":8736,"user":{"displayName":"Antonin Raffin","photoUrl":"","userId":"01724440157307517280"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["# Train the agent for 10000 steps\n","model.learn(total_timesteps=10000)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<stable_baselines.ppo2.ppo2.PPO2 at 0x7feb811cdba8>"]},"metadata":{"tags":[]},"execution_count":8}]},{"metadata":{"id":"ygl_gVmV_QP7","colab_type":"code","outputId":"9af04721-34b0-46d9-cd50-aa67ffdbcb23","executionInfo":{"status":"ok","timestamp":1540023716036,"user_tz":-120,"elapsed":6962,"user":{"displayName":"Antonin Raffin","photoUrl":"","userId":"01724440157307517280"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["# Evaluate the trained agent\n","mean_reward = evaluate(model, num_steps=10000)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mean reward: 151.5 Num episodes: 66\n"],"name":"stdout"}]},{"metadata":{"id":"A00W6yY3NkHG","colab_type":"text"},"cell_type":"markdown","source":["Apparently the training went well, the mean reward increased a lot ! "]},{"metadata":{"id":"9Y8zg4V566qD","colab_type":"text"},"cell_type":"markdown","source":["## Bonus: Train a RL Model in One Line"]},{"metadata":{"id":"iaOPfOrwWEP4","colab_type":"code","colab":{}},"cell_type":"code","source":["model = PPO2('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Se8SHBN17Fy4","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}