{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch2.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"opOgG-C_kR6w","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"psD3uQqPknA7","colab_type":"text"},"cell_type":"markdown","source":["# **Models in PyTorch**\n","\n","\n","*   Supervised Learning Example\n","*   3 classes\n","*   Batch Size 32\n","\n","\n","\n"]},{"metadata":{"id":"y-Nbog5Skgc7","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class TwoLayerNet(nn.Module):\n","  def __init__(self, D_in, H, D_out):\n","    \"\"\"\n","    In the constructor we instantiate two nn.Linear modules and assign them as\n","    member variables.\n","    \n","    D_in: input dimension\n","    H: dimension of hidden layer\n","    D_out: output dimension\n","    \"\"\"\n","    super(TwoLayerNet, self).__init__()\n","    self.linear1 = nn.Linear(D_in, H) \n","    self.linear2 = nn.Linear(H, D_out)\n","  \n","  def forward(self, x):\n","    \"\"\"\n","    In the forward function we accept a Variable of input data and we must \n","    return a Variable of output data. We can use Modules defined in the \n","    constructor as well as arbitrary operators on Variables.\n","    \"\"\"\n","    h_relu = F.relu(self.linear1(x))\n","    y_pred = self.linear2(h_relu)\n","    y_pred = F.relu(y_pred)\n","    return y_pred"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DpQx3103ksQk","colab_type":"code","colab":{}},"cell_type":"code","source":["# N is batch size; D_in is input dimension;\n","# H is the dimension of the hidden layer; D_out is output dimension.\n","N, D_in, H, D_out = 32, 100, 50, 3\n","\n","# Create random Tensors to hold inputs and outputs, and wrap them in Variables\n","x = torch.rand(N, D_in)  # dim: 32 x 100\n","# print(x)\n","\n","y = torch.rand(N, D_out)  # dim: 32 x 3\n","\n","y = torch.FloatTensor(N).uniform_(0, 3).long()\n","    \n","# print(y)\n","\n","# print(y.size())\n","\n","# print(y)\n","\n","# Construct our model by instantiating the class defined above\n","model = TwoLayerNet(D_in, H, D_out)\n","\n","# Forward pass: Compute predicted y by passing x to the model\n","# print(x)\n","y_pred = model(x)   # dim: 32 x 2\n","\n","# print(y_pred.size())\n","\n","# print(y_pred)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GcwPKtGHmLUM","colab_type":"text"},"cell_type":"markdown","source":["# **Loss Function**"]},{"metadata":{"id":"Y3-LE7mslanE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":302},"outputId":"7b8c993e-be9a-4368-929b-5cd88199046b","executionInfo":{"status":"ok","timestamp":1540443628934,"user_tz":420,"elapsed":19956,"user":{"displayName":"Mariano Phielipp","photoUrl":"https://lh5.googleusercontent.com/-4HI6MgZOzBA/AAAAAAAAAAI/AAAAAAAAAAA/rNlgZ1ECWg0/s64/photo.jpg","userId":"05778665983387453541"}}},"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","\n","output = (torch.randn(3,32).float())\n","target = (torch.FloatTensor(3).uniform_(0, 32).long())\n","print(\"Output: \", output)\n","print(\"Target: \" ,target)\n","loss = criterion(output, target)\n","print(\"Loss:\", loss)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Output:  tensor([[ 0.3976, -0.1241, -1.1557,  1.2158,  0.3010,  0.5733,  1.5274,\n","          0.6171,  0.0192, -0.6024, -1.9612, -0.9871, -0.3225, -0.7100,\n","         -1.4539, -0.0773, -1.6230,  0.9777, -0.0581, -1.2614, -2.0150,\n","         -0.8536, -0.7466, -1.2676,  1.1775,  0.8409, -1.2445,  0.6884,\n","         -0.9267, -0.5604, -0.2831, -0.9563],\n","        [-1.6946,  0.1189, -0.5623, -0.6538, -0.3278,  0.5723,  1.1066,\n","         -0.7878, -0.1231, -1.8793, -2.7301, -1.3512,  0.9074, -0.2071,\n","         -1.3011,  1.3194, -1.2997, -0.9457,  0.4361,  0.1044, -1.0863,\n","         -0.3599,  0.0248, -2.0370, -0.8718,  1.2631,  0.1607,  0.3247,\n","         -0.0504, -1.0646, -0.9786,  0.0231],\n","        [-1.8096, -0.7159, -0.1512,  1.3091,  1.1906, -2.5256,  0.3281,\n","          1.6342, -2.2397, -2.8589, -0.2430, -1.3686,  1.1070,  0.2212,\n","         -0.2696,  0.0692,  0.3481,  1.8937, -0.1204,  2.0366,  0.9666,\n","          0.8021, -0.3474,  0.4185,  0.4533, -0.6399, -0.8451, -0.5691,\n","          0.3095,  0.4965, -0.5726, -1.2066]])\n","Target:  tensor([ 18,  22,  29])\n","Loss: tensor(3.5115)\n"],"name":"stdout"}]},{"metadata":{"id":"CWRqQ9MAliWT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":638},"outputId":"a4df44d8-802b-4b49-fcf5-ae25d7f2e8ec","executionInfo":{"status":"ok","timestamp":1540444215577,"user_tz":420,"elapsed":488,"user":{"displayName":"Mariano Phielipp","photoUrl":"https://lh5.googleusercontent.com/-4HI6MgZOzBA/AAAAAAAAAAI/AAAAAAAAAAA/rNlgZ1ECWg0/s64/photo.jpg","userId":"05778665983387453541"}}},"cell_type":"code","source":["print(y_pred)\n","print(y)\n","\n","loss_fn = nn.CrossEntropyLoss()\n","loss = loss_fn(y_pred, y.long())\n","\n","print(\"Torch Loss:\",loss)\n","\n","def userDefinedCrossEntropyLoss(outputs, labels):\n","  # print(\"labels:\",labels)\n","  batch_size = outputs.size()[0]               # batch_size\n","  # print(\"outputs:\",outputs)\n","  outputs = F.log_softmax(outputs,dim=1)       # compute the log of softmax values\n","  # print(\"out log soft:\",outputs)\n","  outputs = outputs[range(batch_size), labels] # pick the values corresponding to the labels\n","  # print(outputs)\n","  return -torch.sum(outputs)/32\n","\n","# print(\"y_pred:\",y_pred.size())\n","# print(\"y:\",y.transpose(0,1).size())\n","loss = userDefinedCrossEntropyLoss(y_pred, y.long())\n","\n","print(\"User Defined Loss:\",loss)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["tensor([[ 0.1854,  0.1120,  0.2549],\n","        [ 0.2387,  0.1056,  0.1274],\n","        [ 0.2683,  0.1179,  0.1758],\n","        [ 0.2713,  0.2277,  0.2346],\n","        [ 0.2350,  0.1633,  0.2247],\n","        [ 0.1813,  0.1541,  0.2876],\n","        [ 0.3718,  0.1811,  0.3602],\n","        [ 0.2109,  0.0810,  0.2994],\n","        [ 0.2384,  0.1889,  0.3350],\n","        [ 0.2563,  0.1073,  0.1847],\n","        [ 0.2669,  0.0180,  0.1705],\n","        [ 0.2854,  0.1335,  0.2481],\n","        [ 0.2529,  0.1296,  0.2761],\n","        [ 0.2567,  0.0962,  0.1453],\n","        [ 0.2781,  0.1755,  0.2639],\n","        [ 0.2648,  0.1215,  0.3435],\n","        [ 0.1928,  0.0000,  0.2761],\n","        [ 0.1133,  0.1907,  0.2261],\n","        [ 0.1249,  0.1229,  0.2809],\n","        [ 0.3003,  0.0637,  0.2137],\n","        [ 0.3414,  0.0000,  0.3098],\n","        [ 0.3668,  0.1756,  0.2765],\n","        [ 0.3543,  0.1161,  0.2686],\n","        [ 0.2542,  0.0709,  0.0332],\n","        [ 0.3362,  0.0498,  0.2985],\n","        [ 0.2972,  0.1284,  0.2641],\n","        [ 0.2859,  0.0524,  0.1965],\n","        [ 0.2683,  0.0643,  0.3081],\n","        [ 0.3612,  0.1742,  0.1903],\n","        [ 0.1860,  0.1423,  0.2054],\n","        [ 0.2271,  0.0580,  0.2594],\n","        [ 0.2734,  0.2221,  0.2954]])\n","tensor([ 0,  0,  1,  2,  1,  1,  2,  1,  1,  2,  1,  2,  0,  0,\n","         1,  2,  1,  0,  2,  2,  0,  2,  0,  2,  1,  2,  1,  0,\n","         1,  1,  1,  2])\n","Torch Loss: tensor(1.1181)\n","User Defined Loss: tensor(1.1181)\n"],"name":"stdout"}]},{"metadata":{"id":"_my3V0gLohpO","colab_type":"text"},"cell_type":"markdown","source":["# **Optimizer**"]},{"metadata":{"id":"lMD-KqMqlh6l","colab_type":"code","colab":{}},"cell_type":"code","source":["# SGD optimizer\n","optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n","\n","# ADAM\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tdeaqPiCotqo","colab_type":"text"},"cell_type":"markdown","source":["# **Training vs Evaluation**\n","\n","Before training the model, it is imperative to call\n","\n","***model.train()***\n","\n","Likewise, you must call\n","\n","***model.eval() ***before testing the model.\n","\n","This corrects for the differences in dropout, batch normalization during training and testing.\n","\n","\n"]},{"metadata":{"id":"vFJZdHW-pdjo","colab_type":"text"},"cell_type":"markdown","source":["# **Core Training Step**"]},{"metadata":{"id":"huj6xbQkopi_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"outputId":"c4ef9afb-82ec-44f4-9695-35d808a50383","executionInfo":{"status":"ok","timestamp":1540444539203,"user_tz":420,"elapsed":844,"user":{"displayName":"Mariano Phielipp","photoUrl":"https://lh5.googleusercontent.com/-4HI6MgZOzBA/AAAAAAAAAAI/AAAAAAAAAAA/rNlgZ1ECWg0/s64/photo.jpg","userId":"05778665983387453541"}}},"cell_type":"code","source":["output_batch = model(x)           # compute model output\n","loss = loss_fn(output_batch, y)   # calculate loss\n","\n","print(loss)\n","\n","for _ in range(5):\n","    \n","    optimizer.zero_grad()  # clear previous gradients\n","    \n","    loss.backward()        # compute gradients of all variables wrt loss\n","\n","    optimizer.step()       # perform updates using calculated gradients\n","    \n","    output_batch = model(x)     \n","    \n","    loss = loss_fn(output_batch, y)\n","\n","    print(loss)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["tensor(1.1181)\n","tensor(1.1159)\n","tensor(1.1137)\n","tensor(1.1116)\n","tensor(1.1095)\n","tensor(1.1075)\n"],"name":"stdout"}]},{"metadata":{"id":"aOGZj_JZpvmw","colab_type":"text"},"cell_type":"markdown","source":["# **Computing Metrics**"]},{"metadata":{"id":"TqDwXeYLp08g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"347898e5-14c4-4f09-a463-18458ff923ec","executionInfo":{"status":"ok","timestamp":1540444643066,"user_tz":420,"elapsed":11769,"user":{"displayName":"Mariano Phielipp","photoUrl":"https://lh5.googleusercontent.com/-4HI6MgZOzBA/AAAAAAAAAAI/AAAAAAAAAAA/rNlgZ1ECWg0/s64/photo.jpg","userId":"05778665983387453541"}}},"cell_type":"code","source":["import numpy as np\n","def accuracy(out, labels):\n","  outputs = np.argmax(out, axis=1)\n","  return np.sum(outputs==labels)/float(labels.size)\n","\n","print(accuracy(output_batch.long().numpy(), y.numpy()))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["0.25\n"],"name":"stdout"}]}]}