{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of multiprocessing_rl.ipynb","version":"0.3.2","provenance":[{"file_id":"1ZzNFMUUi923foaVsYb4YjPy4mjKtnOxb","timestamp":1541340664747},{"file_id":"1_1H5bjWKYBVKbbs-Kj83dsfuZieDNcFU","timestamp":1534416190212}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"hyyN-2qyK_T2","colab_type":"text"},"cell_type":"markdown","source":["# Stable Baselines, a fork of OpenAI Baselines - Easy Multiprocessing\n","\n","Github Repo: [https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)\n","\n","Medium article: [https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82](https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82)\n","\n","## Install Dependencies and Stable Baselines Using Pip\n","\n","List of full dependencies can be found in the [README](https://github.com/hill-a/stable-baselines).\n","\n","```\n","\n","sudo apt-get update && sudo apt-get install cmake libopenmpi-dev zlib1g-dev\n","```\n","\n","\n","```\n","\n","pip install stable-baselines\n","```"]},{"metadata":{"id":"503Gi2076F7u","colab_type":"code","colab":{}},"cell_type":"code","source":["!apt install swig cmake libopenmpi-dev zlib1g-dev\n","!pip install stable-baselines==2.1.1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FtY8FhliLsGm","colab_type":"text"},"cell_type":"markdown","source":["## Import policy, RL agent, ..."]},{"metadata":{"id":"BIedd7Pz9sOs","colab_type":"code","colab":{}},"cell_type":"code","source":["import time\n","\n","import gym\n","import numpy as np\n","\n","from stable_baselines.common.policies import MlpPolicy\n","from stable_baselines.common.vec_env import DummyVecEnv, SubprocVecEnv\n","from stable_baselines.common import set_global_seeds\n","from stable_baselines import ACKTR"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t5WNF6G5gWZ1","colab_type":"text"},"cell_type":"markdown","source":["## Multiprocessing RL Training\n","\n","To multiprocess RL training, we will just have to wrap the Gym env into a SubprocVecEnv object, that will take care of synchronising the processes. The idea is that each process will run an indepedent instance of the Gym env.\n","\n","For that, we need an additional utility function, `make_env`, that will instantiate the environments and make sure they are different (using different random seed)."]},{"metadata":{"id":"TgjfyOTPVxG6","colab_type":"code","colab":{}},"cell_type":"code","source":["def make_env(env_id, rank, seed=0):\n","    \"\"\"\n","    Utility function for multiprocessed env.\n","    \n","    :param env_id: (str) the environment ID\n","    :param num_env: (int) the number of environment you wish to have in subprocesses\n","    :param seed: (int) the inital seed for RNG\n","    :param rank: (int) index of the subprocess\n","    \"\"\"\n","    def _init():\n","        env = gym.make(env_id)\n","        env.seed(seed + rank)\n","        return env\n","    set_global_seeds(seed)\n","    return _init"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iPGIySi2g_RN","colab_type":"text"},"cell_type":"markdown","source":["The number of parallel process used is defined by the `num_cpu` variable.\n","\n","Because we use vectorized environment (SubprocVecEnv), the actions sent to the wrapped env must be an array (one action per process). Also, observations, rewards and dones are arrays."]},{"metadata":{"id":"pUWGZp3i9wyf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"36e104c8-f2cd-4721-bfbd-9e6d8a0f99fd","executionInfo":{"status":"ok","timestamp":1538297590606,"user_tz":-120,"elapsed":2000,"user":{"displayName":"Antonin Raffin","photoUrl":"","userId":"01724440157307517280"}}},"cell_type":"code","source":["env_id = \"CartPole-v1\"\n","num_cpu = 4  # Number of processes to use\n","# Create the vectorized environment\n","env = SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])\n","\n","model = ACKTR(MlpPolicy, env, verbose=0)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n","\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n","\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n","\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"],"name":"stdout"}]},{"metadata":{"id":"4efFdrQ7MBvl","colab_type":"text"},"cell_type":"markdown","source":["We create a helper function to evaluate the agent:"]},{"metadata":{"id":"63M8mSKR-6Zt","colab_type":"code","colab":{}},"cell_type":"code","source":["def evaluate(model, num_steps=1000):\n","    \"\"\"\n","    Evaluate a RL agent\n","    :param model: (BaseRLModel object) the RL Agent\n","    :param num_steps: (int) number of timesteps to evaluate it\n","    :return: (float) Mean reward\n","    \"\"\"\n","    episode_rewards = [[0.0] for _ in range(env.num_envs)]\n","    obs = env.reset()\n","    for i in range(num_steps):\n","      # _states are only useful when using LSTM policies\n","      actions, _states = model.predict(obs)\n","      # here, action, rewards and dones are arrays\n","      # because we are using vectorized env\n","      obs, rewards, dones, info = env.step(actions)\n","      \n","      # Stats\n","      for i in range(env.num_envs):\n","          episode_rewards[i][-1] += rewards[i]\n","          if dones[i]:\n","              episode_rewards[i].append(0.0)\n","\n","    mean_rewards =  [0.0 for _ in range(env.num_envs)]\n","    n_episodes = 0\n","    for i in range(env.num_envs):\n","        mean_rewards[i] = np.mean(episode_rewards[i])     \n","        n_episodes += len(episode_rewards[i])   \n","\n","    # Compute mean reward\n","    mean_reward = round(np.mean(mean_rewards), 1)\n","    print(\"Mean reward:\", mean_reward, \"Num episodes:\", n_episodes)\n","\n","    return mean_reward\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zjEVOIY8NVeK","colab_type":"text"},"cell_type":"markdown","source":["Let's evaluate the un-trained agent, this should be a random agent."]},{"metadata":{"id":"xDHLMA6NFk95","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"fa995d4c-afed-4700-a64e-500a2d2b4b9a","executionInfo":{"status":"ok","timestamp":1538297625840,"user_tz":-120,"elapsed":2709,"user":{"displayName":"Antonin Raffin","photoUrl":"","userId":"01724440157307517280"}}},"cell_type":"code","source":["# Random Agent, before training\n","mean_reward_before_train = evaluate(model, num_steps=1000)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Mean reward: 22.5 Num episodes: 181\n"],"name":"stdout"}]},{"metadata":{"id":"r5UoXTZPNdFE","colab_type":"text"},"cell_type":"markdown","source":["## Multiprocess VS Single Process Training\n","\n","Here, we will compare time taken using one vs 4 processes, it should take ~30s in total."]},{"metadata":{"id":"e4cfSXIB-pTF","colab_type":"code","colab":{}},"cell_type":"code","source":["n_timesteps = 25000\n","\n","# Multiprocessed RL Training\n","start_time = time.time()\n","model.learn(n_timesteps)\n","total_time_multi = time.time() - start_time\n","\n","print(\"Took {:.2f}s for multiprocessed version - {:.2f} FPS\".format(total_time_multi, n_timesteps / total_time_multi))\n","\n","# Single Process RL Training\n","single_process_model = ACKTR(MlpPolicy, DummyVecEnv([lambda: gym.make(env_id)]), verbose=0)\n","\n","start_time = time.time()\n","single_process_model.learn(n_timesteps)\n","total_time_single = time.time() - start_time\n","\n","print(\"Took {:.2f}s for single process version - {:.2f} FPS\".format(total_time_single, n_timesteps / total_time_single))\n","\n","print(\"Multiprocessed training is {:.2f}x faster!\".format(total_time_single / total_time_multi))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ygl_gVmV_QP7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"609418ba-137e-4c62-cb2d-8947f4c583f5","executionInfo":{"status":"ok","timestamp":1537196349118,"user_tz":-120,"elapsed":13206,"user":{"displayName":"Antonin Raffin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"110190111602056379877"}}},"cell_type":"code","source":["# Evaluate the trained agent\n","mean_reward = evaluate(model, num_steps=10000)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Mean reward: 132.5 Num episodes: 302\n"],"name":"stdout"}]},{"metadata":{"id":"QkWsoZ8emt0e","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}